# arxiv_api.py

from fastapi import FastAPI, Query
from typing import List
from pydantic import BaseModel
import requests
from bs4 import BeautifulSoup

app = FastAPI()

# =========================
# ğŸ§  å®šä¹‰è¿”å›æ ¼å¼ï¼ˆæ ‡å‡†åŒ–è¾“å‡ºï¼‰
# =========================
class ArxivResult(BaseModel):
    title: str
    authors: str
    date: str
    abstract: str
    link: str

# =========================
# ğŸš€ æ ¸å¿ƒçˆ¬è™«é€»è¾‘ï¼ˆåŸºæœ¬ä¸å˜ï¼‰
# =========================
def scrape_arxiv(url: str) -> ArxivResult:
    headers = {"User-Agent": "Mozilla/5.0"}
    try:
        resp = requests.get(url, headers=headers, timeout=10)
        soup = BeautifulSoup(resp.text, 'html.parser')

        title = soup.find("h1", class_="title").text.replace("Title:", "").strip()
        authors = soup.find("div", class_="authors").text.replace("Authors:", "").strip()
        abstract = soup.find("blockquote", class_="abstract").text.replace("Abstract:", "").strip()
        date = soup.find("div", class_="dateline").text.strip()

        return ArxivResult(
            title=title,
            authors=authors,
            date=date,
            abstract=abstract,
            link=url
        )
    except Exception as e:
        return ArxivResult(
            title="",
            authors="",
            date="",
            abstract=f"[æŠ“å–å¤±è´¥: {e}]",
            link=url
        )

# =========================
# âœ… API æ¥å£ï¼ˆæ”¯æŒå¤šä¸ª URLï¼‰
# =========================
@app.get("/crawl", response_model=List[ArxivResult])
def crawl_arxiv(
    urls: List[str] = Query(..., description="arXiv é“¾æ¥åˆ—è¡¨")
):
    return [scrape_arxiv(url) for url in urls]
